% ----------------------------------------------------------------------------------------------------------
% Der Ausblick
% ----------------------------------------------------------------------------------------------------------
\section{Ausblick}\label{ausblick}
	Um entstandene Fehler und Schwierigkeiten zu umgehen oder zu beheben gibt es verschiedene Möglichkeiten. Die erste und wichtigste Erweiterung ist es, den Versuchsaufbau anzupassen oder zu ersetzen. Das größte Problem ist die Umsetzung der neuen Position der Laserlinie in die richtige Position der digitalen Punktewolke. Momentan wird sich auf die Genauigkeit des verwendeten Schrittmotors verlassen. So geht man davon aus, dass sich die Linear-Führung um genau einen Millimeter bewegt, um diesen dann bei der neuen Punktewolke mit einzurechnen. Weniger fehleranfällig wäre eine Möglichkeit, die Verschiebung der Punkte zu berechnen. Dazu könnte man die Grundlage des Objektes auf der Linear-Führung mit einer von der Kamera erkennbare Markierung versehen. Das wäre beispielsweise ein ArUco-Marker. Durch die durchgeführte Kalibrierung ist es möglich, ein Koordinatensystem auf den Marker zu legen. Rotation und Translation sind dabei bekannt. Die Punkte der Laserlinie liegen dann in diesem Koordinatensystem, welches die Linear-Führung beschreibt. Als zusätzlicher Schritt müssen die Punkte dann in das Koordinatensystem transformiert werden, in dem die Ebene-Gleichung definiert ist. Alle restlichen Schritte sind bereits implementiert. Durch diese Anpassung wird die neue Punktewolke immer exakt angefügt. Die Länge der Schritte des Schrittmotors müssen dann nur so angepasst werden, dass das Objekt möglichst genau aufgenommen werden kann. Anforderungen an den Aufbau wären dann zusätzlich, dass der verwendete Marker immer auf jedem Bild erkennbar sein muss. \newline
	Grundsätzlich sollte aber das eigentliche Ziel sein, den Versuchsaufbau abzuschaffen und den Sensor an einen Roboterarm anzubringen. Die Bewegung ist ausgelagert und wird vom Client gesteuert. Dieser wird demnach auch die Steuerung des Roboterarms übernehmen. Hier muss der Client an das Gesamtprojekt angepasst werden. \newline
	Ein weiteres Problem ist die Beleuchtung. Hier könnte man nach Vorbild moderner RGB-D-Kameras eine zusätzliche Kamera verwenden. Somit ist eine Kamera für das Aufnehmen der Laserlinie zuständig und eine andere nimmt ein Bild für die Farbinformationen auf. Vorteil davon ist, dass beide Kameras für ihren eigenen Zweck angepasst und eingestellt werden können. Damit umgeht man das Dilemma, dass die Laserlinie schlechter erkennbar wird, um so heller die Oberfläche beleuchtet ist. Ein neu aufkommendes Problem dabei ist, dass die beiden Kameras nicht an der selben Stelle montiert sind. Das bedeutet, dass es zu Fehlern der Farbe kommen kann, da die Positionen der Pixel in den Aufnahmen nicht unbedingt übereinstimmen. Dieses Problem kann behoben werden, indem man die bekannten Transformationen benutzt, um die möglichst genauste Repräsentation eines errechneten 3D-Punktes als Pixel zu finden. Voraussetzung dafür ist die Kalibrierung beider Kameras. Alternativ zur zusätzlichen Kamera wäre die Aufnahme eines dritten Bildes. Das Bildpaar zum Erkennen der Laserlinie behält seine Funktion bei. Hinzu kommt eine dritte Aufnahme, die unter besserer Beleuchtung aufgenommen wird. Dazu müsste man dem Aufbau eine passende Lichtquelle hinzufügen, vergleichbar mit dem Blitzlicht einer Kamera. Diese dritte Aufnahme wird dann über die herkömmliche Methode zum Finden der Farbinformationen weiterverwendet. Vorteil dieser Methode ist, dass der Scanner nun unabhängig von anderer Beleuchtung ist. Alles benötigte Licht wird nun vom Scanner selbst in einer definierten Helligkeit produziert.
	
	Der bisherige Ausblick geht davon aus, dass die Methodik der Lasertriangulation mit einem Linienlaser beibehalten wird. Erwähnenswert sind hierbei Alternativen, wie Stereo Vision oder auch Time of Flight. Die Genauigkeiten eines Scanns mit diesen Methoden und ob diese zu dem Gesamtprojekt passend sind, wurde in dieser Arbeit nicht untersucht. Grundsätzlich ist es sinnvoll, sich jedoch weiter im Bereich der Triangulation aufzuhalten, da Time of Flight ein sehr unterschiedlicher Ansatz ist, 3D-Informationen zu erhalten.
	
	Die zugrunde liegende Aufgabe war es, eine 3D-Rekonstruktion eines Objektes mit Farbinformationen zu erhalten. Diese Aufgabe erfüllt der entwickelte Sensor ausreichend. Die 3D-Informationen werden aus Kamerasicht geliefert. Mithilfe der Farbinformationen können passende Pfade zum Abfahren des Holzes ausgewählt werden.