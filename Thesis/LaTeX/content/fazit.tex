% ----------------------------------------------------------------------------------------------------------
% Das Fazit
% ----------------------------------------------------------------------------------------------------------
\section{Zusammenfassung}\label{zusammenfassung}
	Ziel der Arbeit war es, Oberflächenrekonstruktionen von Objekten mit zusätzlichen Farbinformationen machen zu können. Dieses Ziel wurde erreicht. Eine Kamera und ein Laser bilden mit Software den Lasertriangulationssensor. Dies Kamera nimmt den Linienversatz der Laserlinie auf, der durch die unterschiedliche Höhe des Objektes entsteht. Der Sensor wird dann über das Objekt bewegt und die Oberfläche wird Schritt für Schritt rekonstruiert. Der wichtigste Vorgang ist die initiale Kamerakalibrierung. Durch sie wird die Funktionalität des Sensor gewährleistet und seine Genauigkeit bestimmt. Dabei wird eine Ebene in Form einer Ebenengleichung aus Sicht der Kamera erstellt. Diese stellt das Licht des Linienlasers dar. Durch die bekannte Laserebene können bei einem Scanvorgang die Pixel des Lasers auf die Ebene gespannt werden. Dadurch findet sich die dreidimensionale Repräsentation des Pixels aus Sicht der Kamera. Eine Aufnahme bildet den Querschnitt des Objektes ab. Der Sensor bewegt sich über das Objekt und macht regelmäßig aufnahmen. Dadurch wird die komplette Oberfläche rekonstruiert. \newline
	Zusammengefasst lässt sich festhalten, dass der Lasertriangulationssensor erwartungsgemäß gut funktioniert. Eine einzelne Laserlinie lässt sich mit hoher Genauigkeit als Punktewolke aus Kamerasicht rekonstruieren. Dabei ist die Anwendung mit ROS2 ausführbar. Die größte Fehlerquelle war der Versuchsaufbau selbst. Da nicht genau die Position der Linear-Führung herausgefunden werden kann, können die neuen Laserlinien nie perfekt zu der Gesamt-Punktewolke angefügt werden. Die Implementierung mit ROS2 ist jedoch sehr gut anpassbar. Für die Steuerung des Versuchsaufbau ist grundsätzlich nur der Client zuständig. Die Gesamt-Punktewolke wird vom Sensor selbst erstellt. In der aktuellen Implementation werden die neuen Punkte immer um einen Millimeter in x-Richtung verschoben. Problem war dabei, dass nicht verifiziert werden kann, ob sich die Linear-Führung um genau einen Millimeter bewegt. Wenn man eine Änderung des Versuchsaufbau in Erwägung zieht, muss also der Client selbst und das Anfügen der neuen Punkte in der Scanner-Klasse angepasst werden. Sonst kann der Sensor problemlos weiterverwendet werden. Gerade die entwickelte Bibliothek in Python ist sehr robust. Natürlich ist diese an das verwendetet Kalibrierverfahren angepasst. Hier ist eine Anpassung aufwändiger. Dazu lässt sich jedoch sagen, dass die Genauigkeit einer einzelnen Linie sehr gut ist. Da die Genauigkeit direkt aus der Kalibrierung resultiert, muss im Umkehrschluss die Kalibrierung auch sehr erfolgreich sein. \newline
	Wichtig hervorzuheben ist, dass der entwickelte Sensor an sich immer nur eine einzelne Laserlinie in eine Punktewolke übersetzt. Herkömmliche RGB-D-Kameras können sofort eine ganze Oberfläche mit einer Aufnahme als Punktewolke liefern. Das liegt daran, dass RGB-D-Kameras zumeist über zwei bis drei Kameras verfügen. Zusätzlich benutzten sie nicht nur eine Laserlinie, sondern ganzes Muster an beispielsweise Infrarot-Licht. Ein Beispiel dafür ist die „Intel RealSense d415“ selbst. Der hier entwickelte Lasertraingulationsensor besitzt nur eine Kamera und arbeitet mit einer Laserlinie. Ausgabe ist also immer der Querschnitt des aktuellen Objektes. Trotzdem passt der Sensor damit immer noch zum Gesamtprojekt, die Automatisierung der Jahrringmessstation. Hier soll ein Roboterarm zum Einsatz kommen. Dieser Arm kann sich über das Objekt bewegen. Dabei können genau Befehle an den Arm gesendet werden, um zum Beispiel sich auch genau ein Millimeter zu bewegen. Hinzu kommt, dass bei einem Roboter die Transformationen von der eingebauten Kamera zu einem festgelegten Weltkoordinatensystem bekannt sind. Damit können die errechneten Punkte in das Weltkoordinatensystem transformiert werden und gleich ihren genauen Platz einnehmen. Der Versuchsaufbau, bestehen aus der Linear-Führung und den Arduino mit CNC-Shield ist nur zum Testen des Scannen einer kompletten Oberfläche entwickelt wurden. Das eigentliche Ziel ist jedoch die Anbringung des Sensors an einen Roboterarm. Dafür sind Anpassungen im steuernden Client und in dem Zusammenfügen der Punktewolke notwendig. \newline
	Intern läuft die Zusammenarbeit von ROS2 un der Python-Bibliothek problemlos. Für aufkommende Fehler wurde eine Weiterbehandlung eingebaut. Die hier gemeinten Fehler beziehen sich auf Aufnahmen der Kamera, die beispielsweise kein Schachbrett oder ChArUco-Board zeigen, aber zu Kalibrierung verwendet werden sollen. Also allgemeine Fehler der Bildverarbeitung bei der Kalibrierung. Hierbei wird erkannt, ob eine erfolgreiche Kalibrierung mit den Aufnahmen durchgeführt werden konnte. Falls nicht, wird sie als fehlgeschlagen deklariert. Über das Feedback des aufgerufenen Kalibrier-Service erfährt dann der Benutzer, wo ein Fehler gefunden wurde. Zusätzlich wird bei einer vermeintlich erfolgreichen Kalibrierung die entstandene Laser-Ebene als Punktewolke an Rviz2 geschickt. So kann deren Richtigkeit vom Benutzer überprüft werden. Dabei werden auch die Bilder, die der Sensor zur Kalibrierung aufgenommen hat abgespeichert und können eingesehen werden. Somit erhält der Nutzer genug Informationen über den Erfolg der Kalibrierung. 
