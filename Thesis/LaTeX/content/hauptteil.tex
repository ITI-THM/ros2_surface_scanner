% ----------------------------------------------------------------------------------------------------------
% Eigene Projektbeschreibung und -ergebnisse
% ----------------------------------------------------------------------------------------------------------
\section{Projektumsetzung}\label{ergebnisse}
	\subsection{Kalibrierung}
		Die Kamerakalibrierung ist ausschlaggebend, um die Formel (\ref{eq:basic_trans_complete}) benutzen zu können. Durch sie wird die Kamera-, Rotationsmatrix und der Translationsvektor gefunden. Dabei unterscheidet man zwischen intrinsische und extrinsischer Kalibrierung, bei denen auch ein unterschiedliches Kalibrierverfahren angewandt werden. Die intrinsische Kalibrierung beschäftigt sich mit der Kamera selbst, wobei die extrinsische Kalibrierung die Kamera zu den Laser kalibriert.
		
		\label{chap:kalibierung}
		\subsubsection{Intrinsische Kalibrierung}
		Ergebnis der intrinsischen Kalibrierung ist die Kameramatrix \( A \). Die Kamerakalibrierung ist in OpenCv implementiert und wurde in diesem Projekt genutzt \citep[Vgl.][]{noauthor_opencv_nodate-1}. Die Implementierung in OpenCv richtet sich nach der Kamerakalibrierung nach Zhang \citep{zhang_flexible_2000} und nach Bouguet \citep{noauthor_camera_nodate}. \newline
		Der grundlegende Vorgang ist es, ein bekanntes Muster in verschiedenen Positionen mit der Kamera aufzunehmen. So ein Muster ist beispielsweise ein Schachbrett, welches auch in dieser Arbeit für das Kalibrieren benutzt wurde. Die Maßen des Schachbrettes und der Kacheln sind bekannt. Dabei muss es sich bei der Anzahl der Kacheln nicht um ein herkömmliches Schachbrett handeln. Die Größe und die Menge an Kachel kann zum kalibrieren angegeben werden. Man muss OpenCv also sagen, nach welchem Schachbrett der Algorithmus suchen muss.
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.32\linewidth]{img/hauptteil/calibration/chessboard_corner_0.png}
			\includegraphics[width=0.32\linewidth]{img/hauptteil/calibration/chessboard_corner_1.png}
			\includegraphics[width=0.32\linewidth]{img/hauptteil/calibration/chessboard_corner_2.png}
			%\includegraphics[width=0.39\linewidth]{img/hauptteil/calibration/chessboard_corner_3.png}
			\caption[Schachbrett-Kalibrierung]{Schachbrett-Kalibrierung mit dem Schachbrett aus verschieden Positionen}
			\label{fig:chessboards}
		\end{figure}
	
		Die benutzte Methode heißt \textbf{calibrateCamera()}. Sie nimmt beliebig viele Bilder entgegen. In der eigenen Implementierung wurde gemäß \citep{noauthor_opencv_nodate-1} vorgegangen. Dabei handelt es sich um die offizielle Einführung zur Kamerakalibrierung von OpenCv.In dem Beispiel in Abb. (\ref{fig:chessboards}) wurden der Einführung folgend auch die Ecken des definierten Schachbrettes eingezeichnet. Damit kann man im Grunde sichtbar machen, dass der Algorithmus das Schachbrett richtig erkannt hat. Gemäß \citep{noauthor_opencv_nodate-1} werden mindestens 10 Bilder benötigt, um gute Ergebnisse zu liefern. Daran wurde sich ebenfalls gehalten. Die Methode gibt die folgenden Werte zurück:
		
		$\underline{Kameramatrix}$
		
		Die \textbf{Kameramatrix} wird errechnet und ausgegeben. Dabei handelt es sich um die vollwertige Matrix \( A \) aus den Formeln (\ref{eq:basic_trans}) und (\ref{eq:basic_trans_complete}).
		
		$\underline{Verzerrungs-Koeffizienten}$
		
		Die Verzerrungs-Koeffizienten (eng.: Distortion-Coefficient) werden gemäß \citep{noauthor_opencv_nodate-1} geliefert. Die Verzerrung bezieht sich auf das aufgenommene Bild. Die Auswirkungen sind, dass aufgenommene gerade Linien in der Szene, im Bild nicht mehr gerade dargestellt werden. Sie biegen sich bzw. sind verzerrt. Es gibt zwei verschiedene Verzerrungen, beschrieben in \citep[Vgl.][S. 41]{measurement_1995} und von OpenCv selbst in \citep{noauthor_opencv_nodate-2}.
		
		\begin{figure}[h]
			\centering
			\subfloat[]{\includegraphics[width=0.4\linewidth]{img/hauptteil/calibration/barrel_dist.png} 	\label{subfig:barrel_dist}} 
			\subfloat[]{\includegraphics[width=0.4\linewidth]{img/hauptteil/calibration/pinc_dist.png}
				\label{subfig:pinc_dist}}
			\caption[Verzerrung bei Bildern]{Verzerrung bei Bildern; \ref{subfig:barrel_dist} zeigt die Tonnen-Verzerrung (engl.: Barrel-Distortion) und \ref{subfig:pinc_dist} die Kissen-Verzerrung (engl.: Pincushion-Distortion)}
			\captionsource{Abbildung entnommen aus \citep{measurement_1995}}
			\label{fig:distortion}
		\end{figure}
	
		Um dieser Verzerrung entgegenzuwirken gibt es in OpenCv die Methode \textbf{undistort()}. Diese bringt ein Bild wieder in den normalen Zustand und benutzt dabei die gefunden Koeffizienten \citep[Vgl.][]{noauthor_opencv_nodate-1}. Das benutzen dieser Methode ist unumgänglich. Die Laserlinie ist bei einem ebenen Untergrund gerade. Wenn diese allerdings gebogen ist, führt das beim Aufspannen auf die Ebene zu Höhenunterschieden zwischen den Punkten. Die Verzerrung ist nicht Teil des Pinhole Camera Models und somit auch kein Teil der Formel (\ref{eq:basic_trans}). Bevor mit aufgenommenen Bildern gearbeitet wird, werden die jedoch mit der Methode \textbf{undistort()} entzerrt.
		
		\begin{figure}[h]
			\centering
			\subfloat[]{\includegraphics[width=0.49\linewidth]{img/hauptteil/calibration/distorted.png} 	\label{subfig:distorted}} 
			\subfloat[]{\includegraphics[width=0.49\linewidth]{img/hauptteil/calibration/undistorted.png}
				\label{subfig:undistorted}}
			\caption[Beispiel für die Verzerrung in Bildern]{Hier ein direktes Beispiel aus dem Aufbau. \ref{subfig:distorted} zeigt das Bild vor dem Anwenden von \textbf{undistort()} und \ref{subfig:undistorted} danach.}
			\label{fig:distortion_bsp}
		\end{figure}
	
		Ebenso ist erkennbar, dass es sich bei den verwendeten Kameramodell um eine \textit{Barrel-Distortion} (siehe Abbildung \ref{subfig:barrel_dist}) handelt.
		
		$\underline{Rotationen \; und \; Translationen}$
		
		Die Methode gibt ebenfalls eine Rotationsmatrix und einen Translationsvektor für jedes übergebene Schachbrett zurück. Das sind extrinsische Parameter. In jedes Schachbrett wird ein Koordinatensystem gelegt. Die Rotation und Translation beschreiben dann die Transformation von dem Kamerakoordinatensystem zu diesem Weltkoordinatensystem. Es werden also auch schon extrinsische Parameter geliefert. Diese sind allerdings für den Lasertriangulationssensor selbst nicht ausschlaggebend. Damit dieser mit dem Laser kalibriert werden kann, also eine Ebenen-Gleichung für den Laser gefunden wird, muss der Laser mit auf den Bildern abgebildet sein. Dann kann man die Laserlinie in Bezug auf das gefundene Weltkoordinatensystem weiterverwenden. Für die extrinsische Kalibrierung wurde eine extra Methode entwickelt, welche im nächsten Kapitel beschrieben wird. Die Rotation und Translation zu den verschiedenen Positionen des Schachbrettes sind nicht von Interesse. \newline
		Die intrinsische Kalibrierung ist nur dazu da, die Kameramatrix zu finden und die Distortion-Koeffizienten zu bestimmen, damit die verwendeten Bilder nicht verzerrt sind. Die Kameramatrix ist explizit für die Kamera gültig, mit welcher das Schachbrettmuster aufgenommen wurde. Sie kann also bestimmt werden und ist dann für alle folgenden Rechnungen gültig und unverändert. Genauso verhält es sich mit den Distortion-Koeffizienten. Das bedeutet, dass der Lasertriangulationssensor initial einmal mit dieser Methode kalibriert werden muss, um die intrinsischen Parameter zu bestimmen. Danach werden die Kameramatrix und die Distortion-Koeffizienten abgespeichert und die extrinsische Kalibrierung kann beginnen. 
			\label{chap:kalibrierung_intrinsisch}
			
		\subsubsection{Extrinsische Kalibrierung}
		Die extrinsische Kalibrierung beschäftigt sich allgemein damit, die Kamera zu den eingebauten Linienlaser zu kalibrieren. Ziel dabei ist es, eine Ebenengleichung zu erhalten, die sich im Kamerakoordinatensystem befindet und den Laser repräsentiert. Um sich das besser vorzustellen, kann nochmal Abb. (\ref{fig:lasertriangulation}) oder (\ref{fig:lasertriangulation_position}) eingesehen werden. Hier ist die symbolische Ebene des Lasers rot markiert. Die dazugehörige Gleichung beschreibt dann die Position des Laserlinienstrahls. Grundlegend richtet sich die Methode, um die Ebenen-Gleichung zu finden erneut nach Bajpai und Perelman \citep{bajpai_cross-platform_nodate}. \newline
		Bekannt ist, dass durch eine Kamerakalibrierung die Rotation und Translation herausgefunden werden kann. Durch die intrinsische Kalibrierung ist die Kameramatrix bekannt. Dabei wurde ein Schachbrett aus verschiedenen Positionen aufgenommen. Dabei erhält man durch die Kalibrierung auch Rotation und Translation zu einen Weltkoordinatensystem, welches auf dem Schachbrett platziert wird. Ziel ist es nun ein Schachbrett mit einer Laserlinie aufzunehmen, um die Laserlinie mit dem Weltkoordinatensystem des Schachbrett in Verbindung bringen zu können. Die allgemeine Transformationsformel (\ref{eq:basic_trans_complete}) kann dann für die Laserlinien-Punkte gelöst werden. Mit dem im Kapitel \ref{chap:bildverarbeitung} vorgestellten Verfahren werden die Pixel, die die Laserlinie abbilden gefunden. Diese werden in die Formel (\ref{eq:basic_trans_complete}) eingesetzt. Dabei können die 3D-Punkte errechnet werden, welche sich in dem Weltkoordinatensystem befinden. Aus einer Linie kann jedoch keine Ebene abgeleitet werden. Dazu wird im selben Bild ein weiteres Schachbrett platziert, welches nicht eben, sondern sich in einem gewissen Winkel zu dem ersten befindet. Das zweite Schachbrett hat dabei ein eigenes Weltkoordinatensystem. Da jedoch alle Transformationen bekannt sind, ist es möglich die Laserlinien-Punkte des einen Koordinatensystem in das andere zu transformieren. Betrachtet werden nun zwei Laserlinien, die in einem Winkel zueinander in einem einheitlichen Koordinatensystem stehen. Mit dieser Ausgangslage ist es möglich eine Ebene in die Punkte zu legen. Dabei gibt es diverse Möglichkeiten eine Ebene an Punkte zu fitten und alle liefern eine Ebenengleichung. Theoretisch sind sogar nur mindestens drei Punkte notwendig, um in einem dreidimensionalen Raum eine Ebene zu beschreiben. In diesem Fall sind es weitaus mehr Punkte, die zum Finden der Ebene berücksichtigt werden können. \newline
		In \citep{bajpai_cross-platform_nodate} wird dann bei jedem Durchgang, bei dem eine Laserlinie in eine Punktewolke übersetzt wird, eine aktuelle neue Ebenengleichung erstellt und die Oberflächenpunkte errechnet. Ab diesem Punkt unterscheidet sich diese Arbeit von dem Vorgang im Paper. Die Idee ist es, nach der intrinsischen Kalibrierung eine einmalige extrinsische Kalibrierung vorzunehmen. Es wurde schon erwähnt, dass sich Kamera und Laser in ihrer Position zueinander nicht verändern. Damit bleibt auch die Ebenen-Gleichung in Bezug zur Kamera immer gleich. Die einmal gefundene Ebenengleichung ist also allgemeingültig für den folgenden Scan. Dieser Vorgang der extrinsischen Kalibrierung soll hier nochmal genau beschrieben werden.
		
		$\underline{Zwei \; Schachbretter \; in \; einem \; Bild}$
		
		Um die extrinsische Kalibrierung auszuführen, soll also ein Bild von zwei Schachbrettern gemacht werden. Dabei ist es nötig, dass sie in einem Winkel zueinander stehen. Das hat den Grund, dass die verschiedenen Laserlinien einen Winkel bilden müssen, um darin eine Ebene zu finden. Der Winkel ist dabei grundsätzlich egal. Jedoch sollte kein zu hoher oder zu niedriger Winkel gewählt werden, damit dieser nicht zu eng bzw. zu flach ausfällt. Zu diesem Zweck wurde eine eigene Unterlage designt.
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.49\linewidth]{img/hauptteil/ext-calib/pattern_0.jpg}
			\includegraphics[width=0.49\linewidth]{img/hauptteil/ext-calib/pattern_1.jpg}
			\caption{Unterlage zum Kalibrieren}
			\label{fig:ext-calib-pattern}
		\end{figure} 
	
		Für die Unterlage wurde ein 90°-Winkel benutzt. Das Kalibrier-Brett ist über ein 3D-Drucker gedruckt wurden. Abb. (\ref{fig:ext-calib-pattern}) zeigt Beispielbilder der verwendeten Kalibrier-Unterlage. Dabei ist der Laser auf der Unterlage erkennbar. \newline
		Zusätzlich liefert der entwickelte Lasertriangulationssensor während dem Kalibriervorgang Output-Bilder, die die verschiedenen Arbeitsschritte untermalen. Dabei werden genau die Bilder abgespeichert, die auch für die Bildverarbeitung und Kalibrierung verwendet werden.
		
		\begin{figure}[h]
			\centering
			\subfloat[]{\includegraphics[width=0.49\linewidth]{img/hauptteil/ext-calib/charuco_board.png}
				\label{subfig:raw}}
			\subfloat[]{\includegraphics[width=0.49\linewidth]{img/hauptteil/ext-calib/charuco_board_laser.png}
				\label{subfig:raw_laser}}
			\caption{Ausgangsbilder der extrinsischen Kalibrierung}
			\label{fig:ext-calib-raw}
		\end{figure} 
	
		Gemäß Kapitel \ref{chap:bildverarbeitung} wird ein Bild-Paar aufgenommen. In einem ist die Laserlinie sichtbar (Abb. (\ref{subfig:raw_laser})) und in dem anderen nicht (Abb. (\ref{subfig:raw})).
		
		\newpage
		
		$\underline{Unterscheidung \; der \; Schachbretter}$
		
		In den vorherigen Abbildungen fällt auf, dass es sich bei den verwendeten Kalibriermuster nicht um ein herkömmliches Schachbrett handelt. Das verwendete Muster ist ein ChArUco-Board. Das Problem bei einem herkömmlichen Schachbrett ist, dass der Algorithmus für die Kamera-Kalibrierung in einem einzigen Bild nicht zwischen zwei Schachbrettern unterscheiden kann. Ein ChArUco-Board besitzt in den normal weißen Flächen eines Schachbretts sogenannte ArUco-Marker. Diese Marker können von der Kamera erkannt und eindeutig unterschieden werden. Das Finden und Unterscheiden der ChArUco-Board ist in OpenCv implementiert. Dabei definiert man, ähnlich zu dem Schachbrett aus der intrinsischen Kalibrierung \ref{chap:kalibrierung_intrinsisch}, im Vorhinein die Parameter des Boards. Dazu gehören nicht nur die Kachel, sondern auch eine gewisse ArUco-Bibliothek, welche die Marker beschreibt. Der Algorithmus kann nun in einem Bild die beiden Boards unterscheiden und platziert ein Koordinatensystem darauf. Vorausgesetzt wird dabei, das die Kameramatrix und die Distortion-Koeffizienten bereits bekannt sind. Da die intrinsische Kalibrierung initial durchgeführt wird, ist diese Voraussetzung erfüllt.  
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.49\linewidth]{img/hauptteil/ext-calib/charuco_primary.png}
			\includegraphics[width=0.49\linewidth]{img/hauptteil/ext-calib/charuco_secondary.png}
			\caption{Weltkoordinatensysteme im ChArUco-Board}
			\label{fig:ext-calib-poses}
		\end{figure}
	
		Bekannt sind jetzt die Rotation und Translation von dem Kamerakoordinatensystem zu den zwei Weltkoordinatensystemen, die aus den ChArUco-Boards folgen. Abb. (\ref{fig:ext-calib-poses}) zeigt die Koordinatenachsen. 
		
		$\underline{Finden \; der \; richtigen \; Laserlinie \; im \; Bild}$
		
		Der nächste Schritt muss sein, die Laserlinie im Bild herauszuarbeiten. Dabei sollen die 2D-Punkte der Laserlinie herausgearbeitet und in die 3D-Repräsentation in dem Weltkoordinatensystem umgewandelt werden. Kapitel \ref{chap:bildverarbeitung} beschreibt die Methodik. Das Problem ist, dass nur der Teil der Laserlinie benutzt werden darf, der sich auch tatsächlich auf den jeweiligen ChArUco-Board befindet. Das bedeutet, dass nur ein gewisser Ausschnitt aus dem Bild wichtig ist. Und zwar genau die Laserlinie auf dem zu untersuchenden ChArUco-Board. \newline
		Das ChArUco-Board selbst ist dabei die Lösung des Problems. OpenCv erkennt sowohl die ArUco-Marker, als auch die Ecken der Kachel. Dabei besitzen alle eine eindeutige ID. Über die Ecken kann also ein Bereich abgesteckt werden.
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.8\linewidth]{img/hauptteil/ext-calib/charuco_convex_hull_primary.png}
			\caption{Point of Interest im Bild}
			\label{fig:ext-calib-hull}
		\end{figure}
	
		Um den wichtigen Bereich abzustecken sind die Eckpunkte des Bereiches mit den IDs 0, 5, 18 und 23 ausschlaggebend. Wegen Fehleranfälligkeit wurde jedoch davon abgesehen, genau diese Ecken anzusprechen. Falls in einem Bild eine der äußeren Ecken nicht erkannt wurde, könnten man so den Bereich nicht abstecken. Deswegen wird eine konvexe Hülle benutzt. Damit werden immer die äußersten Punkte berücksichtigt. Fall gewisse Ecken wegen einer Unschärfe im Bild nicht erkannt werden, führt das nun nicht zu Fehlern. In Abb. (\ref{fig:ext-calib-hull}) ist die abgesteckte Fläche in Gelb eingezeichnet. Falls keine Ecke bzw. keine ID gefunden wurden, ist kein ChArUco-Board erkennbar und die Kalibrierung bricht ab. \newline
		Mit den bekannten Bereich kann eine Maske entwickelt werden, die auf das Bild angewandt werden kann. Die Maske besteht dabei aus einem schwarzen Bild, in dem der ausschlaggebende Bereich aus weißen Pixeln besteht. Wenn man das Ziel-Bild bitweise mit der Maske verundet, bleibt nur der markierte Bereich bestehen. Alle andern Pixel sind schwarz. Ausgehend von den Ursprungsbildern (Abb. \ref{fig:ext-calib-raw}) werden für die verschiedenen ChArUco-Boards eine Maske entwickelt und angewandt. Das Ergebnis ist die folgende Abbildung.
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.49\linewidth]{img/hauptteil/ext-calib/charuco_cut_primary.png}
			\includegraphics[width=0.49\linewidth]{img/hauptteil/ext-calib/charuco_cut_secondary.png}
			\caption{Ausgeschnitten Laserlinien}
			\label{fig:ext-calib-cut}
		\end{figure}
	
		Die Achsen des jeweiligen Koordinatensystems sind dabei nur zu Übersicht eingezeichnet und nicht Teil der Bildverarbeitung. Verdeutlicht werden soll jedoch, dass das Weltkoordinatensystem bekannt ist. \newline
		Die erstellte Maske kann auch auf das Ursprungsbild ohne Laser angewandt werden. Die beiden ausgeschnittenen Bilder werden an den Algorithmus zum Finden der Laserlinie (Kapitel \ref{chap:bildverarbeitung}) weitergegeben. Abb. (\ref{fig:ext-calib-laserlines}) zeigt das Ergebnis des Subtrahierens der Bilder. Zur Übersicht sind wieder die Koordinatenachsen eingetragen. Tatsächlich sind an diesem Punkt als Ergebnis der Bildverarbeitung schon die Subpixel der Laserlinie bekannt. Subpixel sind schwierig in einem Bild darzustellen, deshalb wurde sich für das Zeigen des Zwischenschritts entschieden.
		\newpage
		\begin{figure}[h!]
			\centering
			\includegraphics[width=0.49\linewidth]{img/hauptteil/ext-calib/laserline_primary.png}
			\includegraphics[width=0.49\linewidth]{img/hauptteil/ext-calib/laserline_secondary.png}
			\caption{Herausgearbeitete Laserlinien}
			\label{fig:ext-calib-laserlines}
		\end{figure}
	
		$\underline{Der \; Scale-Factor}$
		
		An diesem Punkt kommt es zum Einsatz der Formel (\ref{eq:basic_trans}) für die 3D-Weltkoordinaten. Wie in den Kapitel \ref{chap:transformationen} angemerkt, ist der Scale-Factor noch eine Unbekannte, die gefunden werden muss. Ohne sie wäre die Gleichung noch nicht lösbar. Dieses Problem kann wie folgt umgangen werden. Das Weltkoordinatensystem wird auf das ChArUco-Board gelegt. Die Laserlinie liegt ebenfalls genau auf dem Board. Das bedeutet, dass die Z-Koordinate der Laserlinien-Punkte 0 ist. Zu sehen ist das auch in Abb. \ref{fig:ext-calib-laserlines}. Die Laserlinie liegt in der XY-Ebene. Die Z-Koordinaten der Weltkoordinatenpunkte müssen 0 sein. Mit dem Ausgangspunkt von Gleichung (\ref{eq:pixel_zu_welt}) folgt daraus diese Umstellung:
		
		\begin{equation}
			\begin{aligned}
				p_w &= s \; \vec{a} - \vec{b} \\
				das \; entspricht: \\
				x &= s \; a_x - b_x \\
				y &= s \; a_y - b_y \\
				z &= s \; a_z - b_z \\
				mit \; z &= 0: \\
				0 &= s \; a_z - b_z \\
				s &= \frac{b_z}{a_z}
			\end{aligned}
		\end{equation}
		
		Damit ist die Basistransformationsgleichung lösbar und die 3D-Punkte können errechnet werden. Um nun eine Ebene in die Punkte zu legen, müssen sich alle Punkte in einem einheitlichen Koordinatensystem befinden. Durch die bekannten Transformationen ist dies jedoch kein Problem. Zuerst werden mit (\ref{eq:welt_zu_kamera}) und der eigene Rotation und Translation die Weltpunkte der unteren Laserlinie in das Kamerakoordinatensystem transformiert. Danach wird (\ref{eq:kamera_zu_welt}) genutzt, um mit der Rotation und Translation der oberen Laserlinie die Punkte in das obere Koordinatensystem zu bringen.
	
		$\underline{Das \; Bestimmen \; der \; Ebenengleichung}$
		
		Mit dem letzten Schritt wird die gesuchte Ebenengleichung gefunden. Alle Vorbereitungen sind abgeschlossen. Die richtigen Punkte der zwei nicht parallelen Laserlinien sind in einem einheitlichen Koordinatensystem. Im letzten Schritt muss in diese Punkte eine Ebene gefittet werden. Der Ebenen-Fit wird über die Singular Value Decomposition (SVD) oder auch Singulärwertzerlegung realisiert \citep[Vgl.][S. 349 ff]{liesen_lineare_2021}. Sie ist über die Numpy-Bibliothek in Python implementiert und nutzbar. Die Singulärwertzerlegung wird auf die Punkte angewandt und zerlegt diese in drei Matrizen. Diese beschreiben die am besten passende Lösung zu den übergebenen Punkten. Die erste dieser Matrizen ist per Definition eine orthogonale n x n-Matrix, wobei n die Dimension ist, in der sich die Punkte befinden. Da diese im dreidimensionalen Raum sind, erhält man eine 3 x 3-Matrix. Diese Matrix besteht aus drei Vektoren. Die ersten beiden Spalten-Vektoren spannen die Ebene auf, die am besten zu den übergebenen Punkten passt. Da alle Vektoren in der Matrix orthogonal zueinander sind, liegt der letzte Vektor der Matrix orthogonal auf der Ebene. Damit entspricht der dritte und letzte Vektor der Matrix dem Normalenvektor ($ \vec{n} $) auf der gesuchten Ebene. Mit dem bekannten Normalenvektor kann eine Ebenengleichung als Koordinatenform ermittelt werden. 
		
		\begin{equation}
			\begin{aligned}
				Ax + By +Cz +D &= 0 \\
				\vec{n} &= \begin{pmatrix}
				A \\
				B \\
				C
				\end{pmatrix}
			\end{aligned}
		\end{equation}
		
		Um D zu erhalten, wählt man ein zufälligen Punkt aus den Laserlinien-Punkten aus. Dieser wird dann für x, y und z eingesetzt und D kann errechnet werden. Damit ist das Finden der Laser-Ebene abgeschlossen. 
		\label{chap:kalibrierung_extrinsisch}
		\newpage
		
	\subsection{Das Erzeugen einer Punktewolke}
	Mit Beendigung von diesem Schritt ist die komplette Kalibrierung des Lasertriangulationssensors beendet. Bekannt ist nun die Laserebene. Mit Aufnahme der Laserlinie von einer Oberfläche kann nun eine Punktewolke erzeugt werden.
		\subsubsection{Die 3D-Koordinaten}
	Das Erzeugen der Punktewolke fängt ähnlich an, wie die extrinsische Kalibrierung. Die Grundlage sind wieder zwei Bilder. Aus diesen zwei Bildern werden die Laserlinien-Subpixel nach Kapitel \ref{chap:erkennen_der_laserlinie} errechnet. Diese Pixel werden mithilfe der Ebenengleichung in die tatsächlichen 3D-Punkte aus Sicht des Kamerakoordinatensystem umgewandelt. Grundlage sind wieder die Transformationen aus Kapitel \ref{chap:transformationen}. Bei der extrinsischen Kalibrierung konnte der Scale-Faktor (\( s \)) nur mit der Annahme errechnet werden, dass die resultierenden Punkte in der X-Y-Ebene mit einer Z-Koordinate gleich 0 liegen. Diese Annahme gilt hier nicht mehr. Sicher ist jedoch jetzt, dass die Punkte in der gefundenen Ebenen-Gleichung liegen. Damit gibt es eine vierte geltende Gleichung und alle Unbekannten können errechnet werden. Die folgende Umstellung erläutert die geltenden Gleichungen. Die Ausgangsbasis ist erneut:
	
	\begin{equation}
		\begin{aligned}
			p_w &= s \; \vec{a} - \vec{b} \\
			das \; entspricht: \\
			x &= s \; a_x - b_x \\
			y &= s \; a_y - b_y \\
			z &= s \; a_z - b_z \\
			f\ddot{u}r \; die \; Ebenengleichung \; gilt: \\
			0 &= Ax + By + Cz + D \\
		\end{aligned}
	\end{equation}
	
	Der ausschlaggebende Wert ist der Scale-Faktor. Dieser kann über ein Einsetzten der Koordinaten in die Ebenengleichung bestimmt werden:
	
	\begin{equation}
		\begin{aligned}
			0 &= A(s \; a_x - b_x) + B(s \; a_y - b_y) + C(s \; a_z - b_z) + D \\
			0 &= s \; A \; a_x - A \; b_x + s \; B \; a_y - B \; b_y + s \; C \; a_z - C \; b_z + D \\
			0 &= s \; A \; a_x + s \; B \; a_y + s \; C \; a_z - A \; b_x - B \; b_y - C \; b_z + D \\
			0 &= s(A \; a_x + B \; a_y + C \; a_z) - (A \; b_x + B \; b_y + C \; b_z) + D \\
			\vec{n} &= \begin{pmatrix}
			A \\
			B \\
			C
			\end{pmatrix} \\
			0 &= s(\vec{n} \cdot \vec{a}) - (\vec{n} \cdot \vec{b}) + D \\
			s &= \frac{\vec{n} \cdot \vec{b} - D}{\vec{n} \cdot \vec{a}}
		\end{aligned}
	\end{equation}
	
	Der Scale-Faktor ist damit bekannt und die Koordinaten x, y und z können bestimmt werden. Wichtig hierbei ist, dass die Ebenengleichung in einem Weltkoordinatensystem liegt. Dementsprechend müssen die errechneten 3D-Punkte mit der bekannten Transformation (\ref{eq:welt_zu_kamera}) von dem Weltkoordinatensystem zum Kamerakoordinatensystem transformiert werden.
	
	\subsubsection{Das Finden der Farbinformationen}
	
	Zusätzlich gefordert zu den reinen 3D-Informationen waren auch Farbinformationen. Diese können ebenfalls über das grundlegende Bild-Paar gefunden werden. Das Bild, welches ohne das Einschalten des Lasers aufgenommen wird, beinhaltet die gesuchten Farbinformationen. Gesucht wird nur nach der Farbe an der richtigen Stelle. Die Pixel-Werte der Laserlinie sind bekannt. In dem Ursprungsbild muss an genau der Stelle, an der sich ein Laserpixel befindet die Farbe ausgelesen werden und für den jeweiligen Punkt abgespeichert werden. Problem hierbei ist, dass die Laserpixel Subpixel sind und sich demnach zwischen zwei Pixeln befinden. Für die endgültige Farbe werden dann die zwei umrandenden Pixel gewichtet zusammengerechnet.
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.85\linewidth]{img/hauptteil/bildverarbeitung/weighted_pixel.png}
		\caption{Gewichtete Pixel für den x-Wert 11,4}
		\label{fig:weighted_pixel}
	\end{figure}

	Die Abb. \ref{fig:weighted_pixel} zeigt den Vorgang an einem simplen Beispiel für ein Subpixel in einer beliebigen Reihe (y-Wert) mit einem beispielhaften x-Wert von 11,4. Der nähere Pixel (11) fließt mit höheren Gewicht in die resultierende Farbe ein, als der Pixel weiter weg (12). Die y-Werte der Pixel können keinen ungeraden Wert beinhalten, da der Algorithmus zum Finden der Laser-Pixel (Kapitel \ref{chap:erkennen_der_laserlinie}) jede Pixel-Zeile im Bild durchläuft und dort einen neuen x-Wert errechnet.
	
	Dieser komplette Vorgang transformiert genau eine aufgenommene Laserlinie in eine Punktewolke. Um eine ganze Oberfläche zu rekonstruieren muss sich die Position des Sensors um eine bekannte Länge und Richtung verschieben. Die neu Punktewolke, die aus der neu aufgenommenen Laserlinie resultiert, muss dann entsprechend zu einer Gesamt-Punktewolke ergänzt werden. Um die neuen Punkte korrekt einzufügen, ist es zwingend erforderlich, dass die Bewegung genau bekannt ist. 
	
	\newpage
	
	\subsection{Aufbau - Hardware}
	Für den Lasertriangulationssensor wurde verschiedene Hardware verwendet. Diese soll im Folgendem beschrieben werden. Dabei gibt es zwei verschiedene Abschnitte zu betrachten. Zum einen der Lasertriangulationsensor an sich, der nur in der Lage ist eine Laserlinie in eine Punktewolke umzusetzen. Zum anderen wurde ein Versuchsaufbau angefertigt, um ein Objekt unter dem Sensor entlang zu bewegen. Dabei soll der Sensor regelmäßig eine Punktewolke erstellen. Die zusammengefügte Punktewolke ist dann die Aufnahme der Oberfläche. 
	
		\subsubsection{Hardware-Aufbau des Lasertriangulationssensors}
		Die Hardware des Lasertriangulationssensor besteht grundsätzlich aus einer Kamera und einem Laser.
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.7\linewidth]{img/hauptteil/hardware/aufbau_sensor.png}
			\caption{Der Lasertriangulationssensor}
			\label{fig:aufbau_sensor}
		\end{figure}
		Für die Kamera wurde die Basler a2A1920-160ucPRO verwendet. Dabei handelt es sich um eine Industriekamera mit einer Auflösung von 1920 x 1200 Pixeln. Sie ist eine Farbkamera und kann bis zu 164 Bilder pro Sekunde aufnehmen. Die Spezifikationen sind gemäß \citep{noauthor_a2a1920-160ucpro_nodate} angeben. Wichtig ist, dass die Kamera über USB 3.0 angesprochen werden kann. So kann sie über Software auf einem PC oder ähnlichem angesteuert werden. Mit dem Software-Paket Pylon liefert Basler eine eigene Umgebung um die Kamer zu benutzen und einzustellen. Zusätzlich wird mit Pypylon eine Pythonbibliothek gestellt. Diese gilt als Schnittstelle zwischen Kamera und Python. So kann über direkten Python-Code die Kamera eingebunden werden. \newline
		Das zweite Bauteil ist der Laser. Dabei handelt es sich um einen herkömmlichen roten Laser. Der Lichtstrahl des Lasers wird durch diffraktive optische Elemente (DOE) zu einer Linie geformt. Wie in Abb. \ref{fig:aufbau_sensor} gezeigt, werden dann Kamera und Laser an eine Halterung in entsprechender Position befestigt. \newline
		Die Kamera hat ein ansteuerbares Output-Signal. Dieses Signal kann genutzt werden, um den Laser direkt anzusprechen. Dabei wird ein MOSFET (Metal Oxide Semiconductor Field-Effect Transistors) verwendet. So kann das Output-Signal angesprochen werden, um über den MOSFET den Strom des Lasers an und aus zu stellen. Es ist damit möglich, für die Kamera selbst einen Ablauf zu definieren, in dem sie zuerst ein Bild aufnimmt, dann den Laser anschaltet und anschließend ein zweites Bild aufnimmt. Zuletzt muss der Laser wieder ausgeschaltet werden. So werden von der Kamera die geforderten Bildpaare aufgenommen. Mit Pypylon können diese Bilder in Python und mit OpenCv weiterverwendet werden.
		
		\subsubsection{Der Versuchsaufbau}
		Das Ziel des Versuchsaufbau ist es Bewegung in die aufgenommene Szene zu bringen. Dabei soll die zurückgelegte Bewegung zwischen den Aufnahmen der Oberfläche bekannt sein. So kann die neue Punktewolke der Laserlinie passend angefügt werden. Andernfalls würde man immer den Querschnitt der aktuellen Oberfläche an der Position des Lasers beobachten. Die einfachste Möglichkeit das zu realisieren ist, wenn sich die Szene in genau eine definierte Richtung mit genauen definierten Abstand bewegt. Dazu wurde eine Linear-Führung benutzt. Diese Funktioniert über ein Schrittmotor und bewegt eine Fläche mit einer linearen Bewegung. Der Schrittmotor kann dabei genau angesprochen werden und das Objekt um in unseren Fall genau einen Millimeter weiter schieben.
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.9\linewidth]{img/hauptteil/hardware/aufbau_linearführung.png}
			\caption{Der Lasertriangulationssensor}
			\label{fig:aufbau_scanner}
		\end{figure}
		Die Linear-Führung stammt aus einem 3D-Drucker. Die Software des Lasertriangulationsensor muss die Steuerung des Schrittmotors ansteuern können. Dieser wird über eine CNC-Steuerung mit Arduino und GRBL-Software angesprochen. Der Arduino ist der Mikrocontroller. Die CNC-Steuerung übernimmt das Ansteuern des Motors. Dazu gibt es ein geeignete Erweiterungsplatinen für einen Arduino, sogenannte CNC-Shields. Damit Arduino und CNC-Shield miteinander kommunizieren können wird die Open-Source-Software GRBL verwendet. So können Befehle des Arduino über GRBL und der CNC-Steuerung für den Schrittmotor lesbar sein.
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.45\linewidth]{img/hauptteil/hardware/arduino.png}
			\caption{Arduino mit CNC-Shield}
			\label{fig:arduino}
		\end{figure}
		Mit diesem Aufbau kann über die Bibliothek \textit{serial} in Python eine sogenannte \glqq Serial Connection\grqq{} aufgebaut werden. Damit kann direkt über ein USB-Anschluss auch der Motor angesprochen werden. Über die Serial Connection werden dann GRBL-Befehle an den Motor gesendet. Diese sind einfache Befehle wie \glqq Startposition festlegen!\grqq{}, \glqq 1mm bewegen\grqq{} oder \glqq Zur Startposition zurückkehren!\grqq{}.
		
	
	\label{chap:aufbau_hardware}
	
	\newpage
	
	\subsection{Aufbau - Software}
	Die Software des Lasertriangulationssensor gliedert sich in zwei Bereiche. Zum einen wurde eine Python-Bibliothek entwickelt. Diese wurde objektorientiert mit diversen Klassen, die den Lasertriangulationssensor repräsentieren entwickelt. Grundlegende Aufgabe der Python-Bibliothek sind die elementaren Berechnungen, um eine Punktewolke aus einem Bild-Paar zu erhalten. Das bedeutet, dass ein Konstrukt erstellt werden kann, welches mit der Eingabe eines Bildpaares eine Punktewolken liefert. \newline
	Der andere große Bereich ist die Umsetzung in ROS2. ROS2 liefert das Betriebssystem des Scanners. Alle verschiedenen Elemente sollen zu einem Gesamtprodukt zusammengefügt werden. Dazu gehört die Ansteuerung der Kamera und des Lasers. Schwerpunkt ist auch die Kommunikation zwischen einzelnen Elementen. Beispiele sind dabei das Aufnehmen der Bilder für die Kalibrierung oder das Erstellen der Punktewolke und die Weiterleitung an die entsprechenden Methoden aus den Klassen der Python-Bibliothek.
		\subsubsection{Python (Bibliothek)}
		Die Python-Bibliothek besteht grundlegend aus vier Klassen. Die Scanner-, Kamera-, Laser-, und LaserLine-Klasse. Dabei ist die Scanner-Klasse die volle Repräsentation des Lasertriangulationssensors, die in ROS2 eingebunden wird.
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.85\linewidth]{img/hauptteil/software/Scanner_UML.png}
			\caption{Die Scanner-Klasse als UML-Diagramm}
			\label{fig:scanner_uml}
		\end{figure}
		Der Scanner beschreibt den Lasertriangulationssensor. Er besitzt eine Kamera und einen Laser. Zusätzlich wird als Variable die Punktewolke der Oberfläche und auch direkt deren Punkte und Farben abgespeichert. Hinzu kommt ein Boolean-Flag um zu kennzeichnen, ob der Scanner kalibriert ist. Der xStep ist für den Versuchsaufbau wichtig. Über die Liniarführung wird das Objekt in x-Richung unter dem Sensor durch bewegt. Die jeweiligen Millimeter-Schritte müssen entsprechend einbezogen werden. Dazu wird diese Variable genutzt. \newline
		Zusätzlich stellt die Scanner-Klasse die grundlegenden Methoden, die die Funktionalität des Triangulationssensors widerspiegelt zur Verfügung. Diese umfassen zum einen das komplette kalibrieren des Scanners. Dazu wird eine Liste von Bildern für die intrinsische Kalibrierung und ein Bildpaar des ChArUco-Boards für die extrinsische Kalibrierung benötigt. Hinzu kommt eine abgewandelte Version der Kalibrier-Methode, in der die intrinsische Kalibrierung durch das Importieren der Kameradaten ersetzt wird. Zusätzlich gibt es eine Methode zum Erstellen einer Punktewolke. Sie bekommt ein entsprechendes Bildpaar von der Oberfläche übergeben und errechnet die Punktewolke. Wenn der Scanner sich gerade in einem Scanvorgang befindet wird die neue Punktewolke entsprechend an die gesamte Punktewolke angehängt. Die Scanner-Klasse besitzt zusätzlich noch Hilfsmethoden die Zugriff von außen auf den Scanner Status und die Punktewolke geben oder die Punktewolke selbst erneuern oder aktualisieren.
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.7\linewidth]{img/hauptteil/software/Kamera_UML.png}
			\caption{Die Kamera-Klasse als UML-Diagramm}
			\label{fig:kamera_uml}
		\end{figure}
		Die Kamera besitzt ebenfalls eine Repräsentation in Form einer Klasse. Diese Klasse fungiert als Speicher für die Kameradaten. Gespeichert werden die Kamera-Matrix, die Distortion-Koeffizienten und die Rotationsmatrizen, sowie die Translationsvektoren der Bilder von der intrinsischen Kalibrierung. Also alle Daten, die aus der intrinsische Kalibrierung hervorgehen. Hierbei ist wichtig hervorzuheben, dass die Klasse keine Bilder aufnehmen kann bzw. keinen Zugriff auf die tatsächliche Kamera im Triangulationssensor hat. Sie is lediglich für das Ablegen der Daten und die intrinsische Kalibrierung zuständig. Zusätzlich beinhaltet sie den Kalibrier-Algorithmus, um die Kameradaten zu erzeugen. 
		\newpage
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.35\linewidth]{img/hauptteil/software/LaserLine_UML.png}
			\caption{Die LaserLine-Klasse als UML-Diagramm}
			\label{fig:laser_line_uml}
		\end{figure}
		Die Klasse LaserLine beschreibt eine aufgenommene Laserlinie. Information, die als Variablen gespeichert werden, sind das Differenz-Bild, die errechneten 3D-Punkte der Linie und die entsprechende Rotation und Translation zu dem Weltkoordinatensystem in dem sich die Laserlinie befindet. Zum Initialisieren der LaserLine-Klasse wird das aufgenommene Bildpaar und die Rotation und Translation zu dem jeweiligen Weltkoordinatensystem benötigt. Die Klasse errechnet zusätzlich im Konstruktor die Bild-Differenz und die Laserlinienpunkte.
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.35\linewidth]{img/hauptteil/software/Laser_UML.png}
			\caption{Die Laser-Klasse als UML-Diagramm}
			\label{fig:laser_uml}
		\end{figure}
		So wie die Kamera-Klasse die Daten der intrinsischen Kalibrierung beinhaltet, beinhaltet die Laser-Klasse die Daten der extrinsischen Kalibrierung. Dazu benötigt sie zwei Laserlinien der Klasse LaserLine, die zur Unterscheidung als \glqq oben\grqq{} und \glqq unten\grqq{} definiert sind. Aus diesen kann die Ebenengleichung bestimmt werden. Dazu wird eine eigene Methode bereitgestellt, in der erst die Punkte in ein einheitliches Koordinatensystem zusammengefasst werden und darauf folgend dann eine Ebene an die Punkte gefittet wird. Die vereinheitlichten Punkte und die Ebenengleichung werden von der Klasse gespeichert.
		\newpage
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.7\linewidth]{img/hauptteil/software/Hilfsmethoden_UML.png}
			\caption{Die Hilfsmethoden ohne Zugehörigkeit zu einer Klasse}
			\label{fig:hilfsmethoden_uml}
		\end{figure}
		Zusätzlich zu den vier beschriebenen Klassen gibt es Hilfsmethoden, die nicht konkret zu einer Klasse gehören. Das bedeutet, dass sie nicht nur von einer Klasse benutzt werden, sondern von verschiedenen Klassen an verschiedenen Stellen aufgerufen werden. Diese Methoden umfassen die Transformationen zwischen den Koordinatensystemen, die Methoden für das Plane-Fitting und die Berechnung der Subpixel aus einem Differenz-Bild.
		
		Die gesamte Darstellung des UML-Diagramm befindet sich im Anhang unter \ref{anhang-a}.
		
		\subsubsection{ROS2}
		
		Übergeordnet über der Python-Bibliothek befindet sich die ROS2-Implementierung. Damit wird letztendlich der Triangulationssensor bedient. Im Anhang unter \ref{anhang-b} wird die Umsetztung in ROS2 visualisiert dargestellt. \newline
		Mit ROS2 erstellt man sogenannte \textbf{Nodes}, die verschiedene Möglichkeiten bereitstellen, miteinander zu kommunizieren. Dazu gehören zum einen \textbf{Services}. Services sind am ehesten mit Methoden einer Klasse zu vergleichen, wobei dann die Nodes eine Klasse darstellen. Eine Node stellt ein Service bereit. Dieser kann aufgerufen werden und man erhält eine Antwort zurück. Der Triangulationssensor selbst ist auch über Nodes umgesetzt. Dabei sind die beiden ausschlaggebenden Nodes die Scanner-Node und die Kamera-Node. Die Scanner-Node enthält die vorgestellte Python-Bibliothek und damit eine Instanz von der Scanner-Klasse. Die Instanzen von der Scanner-Klasse selbst, also der Laser und die Kamera sind nach Initialisierung noch nicht mit Daten gefüllt. Das liegt daran, dass die Kalibrierung noch nicht stattgefunden hat. Dazu gibt es die Kamera-Node, die nun konkrete Bilder liefern kann. Die Kamera-Node hat Zugriff auf die Pypylon-Bibliothek und damit auf die Repräsentation der Basler-Kamera. Die Node beinhaltet eine Funktion, die das gewünschte Bildpaar liefert. Da der Linienlaser selbst über den Output der Kamera gesteuert wird, kann dieser auch direkt in der Kamera-Node angesprochen werden. So stellt die Kamera ein Service bereit, der ein Bildpaar aufnimmt und dieses an den Scanner weiterleiten kann. Genauso stellt sie einen Service bereit, der mehrere Bilder aufnimmt und diese als Liste weiterleitet. Damit ist auch für die intrinsische Kalibrierung gesorgt. \newline
		Diese Kommunikation funktioniert in ROS2 über \textbf{Topics}. Topics beschreiben eine Nachricht. Eine Node hat die Möglichkeit eine Nachricht über ein gewähltes Topic zu \linebreak veröffentlichen (publish). Ebenfalls gibt es die Möglichkeit , dass eine Node Nachrichten über ein gewähltes Topic empfängt (subscribe). Der Service der Kamera nimmt ein Bildpaar oder eine Liste an Bildern auf und \linebreak veröffentlicht sie über das passende Topic. \newline
		In der Kommunikation muss zusätzlich festgehalten werden, welchen Datentyp die Nachricht bzw. das Topic verwenden soll. In ROS2 wird das über einen \textbf{Message}-Type realisiert. Hierbei kann man selbst eine Message erstellen oder man benutzt Vorgefertigte, die von ROS2 bereitgestellt werden. Für ein Bild gibt es einen vorgefertigten Message-Type. Da jedoch zwei Bilder auf einmal geschickt werden sollen, wurde hier ein eigener Typ (img\_pair.msg) erstellt. Ähnlich verhält es sich bei dem Schicken der Kalibrierungsbilder für die intrinsische Kalibrierung. Hier wird ein Message-Type erstellt, der eine Liste an Bildern beinhaltet (cam\_calib\_imgs.msg). Die Kamera nimmt demnach bei einem Aufruf des Services die Bilder auf und veröffentlicht sie über das jeweilige Topic. Der Scanner ist ein Subscriber zu diesem Topic und empfängt sie. Dabei wird in der Node eine Callback-Funktion aufgerufen, die auf dieses Empfangen reagiert. Der Scanner selbst speichert die für die Kalibrierung gesendeten Bild erst einmal nur ab. Zusätzlich hat ein Service in ROS2 die Möglichkeit ein Feedback zurückzugeben. Diese Möglichkeit wird hier genutzt, damit die Kamera zeigen kann, dass die Bilder erfolgreich aufgenommen und veröffentlicht wurden.\newline
		Die Scanner-Node stellt einen eigenen Service bereit, der den Scanner kalibriert. Dazu werden die Methoden, die die Scanner-Klasse bereitstellt genutzt. Überprüft wird hierbei, ob dem Scanner schon die benötigten Bilder bereitstehen. Falls das nicht der Fall ist, wird über die Antwort des Services darauf hingewiesen und der Aufruf des Services schlägt fehl. Andernfalls kann die Node den Scanner Kalibrieren und ein Scan kann gestartet werden.
		
		In der ROS2-Implementierung gibt es noch zwei weitere Nodes. Zum einen wird ein Empfänger gebraucht, der entsprechende Ergebnisse des Scanners auch empfangen und benutzen kann. In diesem Fall werden von Scanner Punktewolken veröffentlicht. Das geschieht ebenfalls über ein Topic. Dabei ist der Message-Type des Topics ein von ROS2 vorgefertigter Typ, der Punktewolken beinhaltet. Benötigt demnach wieder eine Node, die ein Subscriber des Punktewolken-Topic ist. Dazu wird \textbf{Rviz2} benutzt. Rviz2 ist ein von ROS2 bereitgestellten Paket und dient zur Visualisierung. Die Funktionalität von Rviz2 ist sehr weitgehend. Die Hauptaufgabe von Rviz2 in diesem Projekt ist es, Subscriber von dem Punktewolken-Topic zu sein. Die Empfangenen Punktewolken können hier dargestellt und dynamisch beobachtet werden. \newline
		Die vierte Instanz, die benötigt wird, sind sogenannte Clients. Es wurde schon öfters von Services und dem Aufrufen eines Service geredet. In ROS2 wird dafür ebenfalls eine Node benötigt, die den jeweiligen Service aufruft und das entsprechende Feedback erhält. In der Anlage unter \ref{anhang-b} werden die Client-Nodes als eine einzelne Node zusammengefasst. Intern wird für jeden Aufruf eines Services jedoch eine eigene individuelle Node generiert. Da diese allerdings alle den selben Zweck folgen, den bestimmten Service aufrufen und das Feedback ausgeben, wurden sie hier als eine einzelne Node zusammengefasst. Als Nutzer startet man die jeweiligen Clients, um das gewollte System zu starten. Voraussetzung dabei ist, dass die Service-Nodes, hier der Scanner und die Kamera, ebenfalls gestartet und aktiv sind. Diese werden genauso, wie ein Client gestartet, nur dass sie selbst nicht aktiv etwas tun, es sei denn der Service, den sie bereit stellen wird aufgerufen oder ein Topic von dem sie Subscriber sind wird veröffentlicht. Sie warten also auf eine Interaktion. Die Client-Nodes sind dabei die Akteure, die eine Interaktion hervorrufen. Das Starten der Nodes ist in ROS2 über Konsolen-Skripts umgesetzt. Die wichtigen Skripts, die von einer gewissen Aktionskette gefolgt sind, sollen im Folgenden beschrieben werden. In \ref{anhang-b} wurden diese Skripts als Funktionen (Functions) benannt, da sie nicht einfach nur eine Node starten, sondern im ganzen System etwas zum Laufen bringen.
		
		$\underline{Vorgang\;bei\;der\;Kalibrierung\;des\;Lasertriangulationssensors}$
		
		Die jetzt beschriebenen Vorgänge sind im Anhang unter \ref{anhang-c} in einem Sequenz-Diagramm visualisiert. \newline
		Der Scanner brauch für eine Kalibrierung Bilder für die intrinsische und für die extrinsische Kalibrierung. Da sich die Szene der Bilder ändert, findet zwischen den Kameraaufnahmen keine Automatisierung statt. Das bedeutet, dass manuell zuerst die Bilder-Liste der intrinsischen Kalibrierung und das Bild-Paar der extrinsischen Kalibrierung von der Kamera geschickt werden muss, bevor die Kalibrierung selbst erfolgen kann. Welche Aufnahme zuerst ausgeführt wird, ist dabei nicht wichtig. In \ref{anhang-c} wird zuerst die Funktion \verb|intr_calib_imgs| aufgerufen. Diese startet ein Client, der den Service der Kamera \verb|send_cam_calib_imgs| aufruft. Die Kamera-Node benutzt daraufhin ihre Funktion zum Aufnehmen der Kalibrierbilder. Der Benutzer muss dann für jedes Bild das Schachbrett in einer anderen Position vor die Kamera halten. Die Kamera-Node veröffentlicht die Bild-Liste und schickt ein positives Feedback an den Client zurück. Der Client gibt das Feedback in der Konsole aus und ist damit fertig. Die Node hört somit auf zu existieren. Der Scanner reagiert auf das veröffentlichen der Bilder und speichert sie ab. \newline
		Der selbe Vorgang passiert bei der Funktion \verb|extr_calib_imgs|. Der Unterschied besteht darin, dass der Client ein anderen Service (\verb|send_laser_calib_imgs|) aufruft. Der Benutzer muss hier das Kalibrier-Brett mit dem ChArUco-Board vor die Kamera halten. \newline
		Der Scanner ist nun im Besitzt der benötigten Bilder. Mit der Funktion \verb|calibrate| wird ein Client gestartet, der den Service \verb|calibrate_laser| der Scanner-Node aufruft. Das veranlasst die Scanner-Node dazu, die Funktion \verb|calibrate_scanner()| der internen Scanner-Klasse aufzurufen. Für die Visualisierung und zum Überprüfen erstellt die Scanner-Node eine Punktewolke, die die Laserebene darstellt und veröffentlicht sie über ein Topic. Über Rviz2 kann dann das Ergebnis der Kalibrierung angesehen werden.
		
		$\underline{Vorgang\;des\;Scannen}$
		
		Die jetzt beschriebenen Vorgänge sind im Anhang unter \ref{anhang-d} in einem Sequenz-Diagramm visualisiert. \newline
		Nachdem der Scanner kalibriert ist, kann ein Scann-Vorgang, zugeschnitten auf den Versuchsaufbau vorgenommen werden. Hierbei wird kein konkreter Service der Scanner-Node benutzt. Die Scanner-Node weiß viel mehr, dass das gesendete Bild-Paar von der Oberfläche aufgenommen wurde und eine Punktewolke errechnet werden soll. Das verwendete Topic (\textit{ImgPair}) hat einen Message-Type, der zwei Bilder beinhaltet. Dieser kann somit sowohl für die extrinsische Kalibrierung, als auch für ein Bildpaar von der Oberfläche genutzt werden. Hier wurde ein Boolean-Flag eingebaut, der zeigt, ob das Bild-Paar für die Kalibrierung oder zur Oberflächenrekonstruktion benutzt werden soll. Somit weiß die Scanner-Node genau, wie sie weiterzuarbeiten hat. Falls die Scanner-Node ein Bildpaar zur Oberflächenrekonstruktion erhält, aber nicht kalibriert ist, wird der Fehlerfall mit einer entsprechenden Warnung behandelt. \newline
		Die Funktion \verb|start_scan| starten einen Client, der zum Anfang der Scanner-Node über das Topic ScannerStatus signalisiert, dass sie sich jetzt im laufenden Scan befindet. Der Scan startet damit, dass der Service \verb|send_img_pair_surface| der Kamera aufgerufen wird. Die Kamera nimmt ein Bild-Paar auf und veröffentlicht es. Der Scanner reagiert darauf, in dem er das Bildpaar in eine Punktewolke umsetzt. Diese wird dann ebenfalls veröffentlicht und in Rviz2 dargestellt. Mit dem Senden des Bild-Paares schickt die Kamera ein positives Feedback an den Client zurück. Der Client signalisiert dann der Serial-Connection über die Sprache Grbl, dass sich die Linearführung um einen Millimeter bewegen soll. Danach wiederholt sich der Vorgang mit dem Aufrufen des Services der Kamera. Dabei bestimmt der Client wie oft sich der Vorgang wiederholt. Im Versuchsaufbau bewegt sich die Linear-Führung 290mm, somit wird der Vorgang 290 mal wiederholt. Die Scanner-Node selbst zählt auch mit, wie oft ein Bild-Paar veröffentlicht wurde und rechnet die Verschiebung mit ein. Die aktualisierte Punktewolke wird dabei immer selbst neu veröffentlicht und in Rviz2 aktualisiert. \newline
		Nachdem alle Wiederholungen durchgeführt wurden signalisiert der Client der Scanner-Node, dass der Scan fertig ist. Daraufhin wird der Scanner die finale Punktewolke in einem (.ply)-File speichern und dann die intern gespeicherte Punktewolke auf null setzten. Damit ist er bereit führ einen erneuten Scan und behält seine Kalibrierung bei.
		
		\newpage
		
		
	\subsection{Qualitative Ergebnisse}\label{chap:qual_ergeb}
	
	In diesem Abschnitt sollen qualitative Ergebnisse von einem Scan-Vorgang gezeigt werden.
	
	\begin{figure}[h]
		\centering
		\subfloat[]{\includegraphics[width=0.49\linewidth]{img/hauptteil/scan-imgs/scan_img_0.png}
			\label{subfig:scan_0}}
		\subfloat[]{\includegraphics[width=0.49\linewidth]{img/hauptteil/scan-imgs/scan_img_1.png}
			\label{subfig:scan_1}} \\
		\subfloat[]{\includegraphics[width=0.49\linewidth]{img/hauptteil/scan-imgs/scan_img_2.png}
			\label{subfig:scan_2}}
		\subfloat[]{\includegraphics[width=0.49\linewidth]{img/hauptteil/scan-imgs/scan_img_3.png}
			\label{subfig:scan_3}}
		\caption[Qualitative Ergebnisse]{Schachbrett-Kalibrierung mit dem Schachbrett aus verschieden Positionen}
		\label{fig:scan_imgs}
	\end{figure}
	Die aufgenommenen Bilder stammen aus Rviz2. Im Anhang unter \ref{anhang-e} befinden sich weitere Aufnahmen, in dem der Scan näher betrachtet wird. Dabei kann man genau die einzelnen Linien erkennen, die zusammengefügt die komplette Oberflächenrekonstruktion ergeben.